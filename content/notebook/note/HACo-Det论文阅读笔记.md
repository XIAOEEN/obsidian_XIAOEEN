# HACo-Det: 人机协作写作场景下的细粒度机器生成文本检测

## 基本信息

- **标题**: HACo-Det: A Study Towards Fine-Grained Machine-Generated Text Detection under Human-AI Coauthoring
- **作者**: Hao Liang, Linzhuang Sun, Jingwei Yi, Yifan Liu, Binglin Zhou, Liping Jing
- **机构**: Beijing Jiaotong University, Zhejiang University, Chinese Academy of Sciences
- **发表时间**: 2025
- **论文链接**: [待补充]
- **代码**: https://github.com/Dongping-Chen/HACo-Det

## 研究背景

### 问题动机

随着大语言模型（LLMs）的快速发展，人机协作写作（Human-AI Coauthoring）已成为内容创作的主流模式。然而，现有的机器生成文本检测（MGT Detection）研究主要关注**二分类任务**（人类 vs. 机器），无法应对以下实际场景：

1. **混合文本检测**：真实场景中，文本往往是人类和AI共同创作的混合体
2. **细粒度归属**：需要识别文本中哪些部分由人类撰写，哪些由AI生成
3. **修改检测**：AI生成的内容可能被人类进一步修改，需要识别这种修改行为

### 现有方法的局限性

- **传统MGT检测器**：仅能判断整篇文本是否由机器生成，无法定位具体位置
- **缺乏细粒度标注数据集**：现有数据集缺少句子级或段落级的归属标注
- **忽略人类修改行为**：未考虑人类对AI生成内容的后续编辑

## 主要贡献

1. **新任务定义**：首次系统性地定义了人机协作写作场景下的细粒度MGT检测任务，包含三个子任务：
   - **边界检测（Boundary Detection）**：识别人类撰写和机器生成内容的边界
   - **作者归属（Authorship Attribution）**：判断每个文本片段的作者（人类/机器）
   - **修改检测（Modification Detection）**：识别机器生成内容是否被人类修改

2. **HACo-Bench数据集**：构建了首个细粒度标注的人机协作写作数据集
   - 规模：9,792个样本
   - 覆盖领域：新闻、学术论文、创意写作、问答等
   - 标注粒度：句子级归属标签 + 修改标签
   - 生成模型：GPT-3.5, GPT-4, Claude, Gemini等主流LLMs

3. **HACo-Det框架**：提出统一的检测框架，整合三个子任务
   - 基于序列标注的方法
   - 多任务学习架构
   - 支持端到端训练

4. **全面评估**：对比了多种基线方法，包括传统MGT检测器、序列标注模型、大模型等

## 方法论

### 任务形式化定义

给定一个文本序列 $D = \{s_1, s_2, ..., s_n\}$（由n个句子组成），目标是预测：

1. **边界检测**：识别作者切换点 $B = \{b_1, b_2, ..., b_k\}$
2. **作者归属**：为每个句子分配标签 $A = \{a_1, a_2, ..., a_n\}$，其中 $a_i \in \{\text{Human}, \text{Machine}\}$
3. **修改检测**：为机器生成的句子标注是否被修改 $M = \{m_1, m_2, ..., m_n\}$，其中 $m_i \in \{\text{Original}, \text{Modified}, \text{N/A}\}$

### HACo-Det框架架构

框架采用**序列标注（Sequence Labeling）**范式，主要包含以下组件：

#### 1. 编码器（Encoder）

- 使用预训练语言模型（如RoBERTa, DeBERTa）作为骨干网络
- 输入：句子序列的token embeddings
- 输出：句子级表示向量 $\{h_1, h_2, ..., h_n\}$

#### 2. 多任务预测头（Multi-task Heads）

**边界检测头**：
- 二分类任务，预测相邻句子间是否存在作者切换
- 输入：$[h_i; h_{i+1}]$（拼接相邻句子表示）
- 输出：边界概率 $P(b_i)$

**作者归属头**：
- 二分类任务，预测每个句子的作者
- 输入：句子表示 $h_i$
- 输出：作者标签 $P(a_i)$

**修改检测头**：
- 三分类任务（原始/修改/不适用）
- 仅对机器生成的句子进行预测
- 输入：句子表示 $h_i$ + 上下文信息
- 输出：修改标签 $P(m_i)$

#### 3. 训练目标

采用多任务学习，联合优化三个子任务：

$$\mathcal{L}_{total} = \lambda_1 \mathcal{L}_{boundary} + \lambda_2 \mathcal{L}_{authorship} + \lambda_3 \mathcal{L}_{modification}$$

其中：
- $\mathcal{L}_{boundary}$：边界检测的交叉熵损失
- $\mathcal{L}_{authorship}$：作者归属的交叉熵损失
- $\mathcal{L}_{modification}$：修改检测的交叉熵损失
- $\lambda_1, \lambda_2, \lambda_3$：任务权重超参数

## HACo-Bench数据集构建

### 数据收集策略

数据集构建采用**模拟真实协作场景**的方式，包含以下几种协作模式：

1. **人类起草 + AI续写**：人类撰写开头，AI继续完成
2. **AI起草 + 人类修改**：AI生成初稿，人类进行编辑和润色
3. **交替写作**：人类和AI轮流撰写段落
4. **AI生成 + 人类插入**：在AI生成的文本中插入人类撰写的内容

### 数据集统计

| 维度 | 数值 |
|------|------|
| 总样本数 | 9,792 |
| 平均句子数/样本 | 15.3 |
| 人类句子占比 | 42.6% |
| 机器句子占比 | 57.4% |
| 修改句子占比 | 18.2% |
| 涉及LLMs | GPT-3.5, GPT-4, Claude-2, Gemini-Pro |
| 领域分布 | 新闻(25%), 学术(20%), 创意写作(30%), 问答(25%) |

### 标注流程

1. **自动标注**：利用协作写作过程中的元数据自动生成初始标签
2. **人工校验**：由3名标注员进行交叉验证
3. **质量控制**：Cohen's Kappa = 0.82（高一致性）

## 实验结果

### 基线方法对比

论文对比了以下几类方法：

1. **传统MGT检测器**：DetectGPT, GLTR, GPTZero（适配为句子级检测）
2. **序列标注模型**：BiLSTM-CRF, BERT-CRF
3. **大语言模型**：GPT-4, Claude-3（通过prompt进行零样本检测）
4. **HACo-Det变体**：单任务版本 vs. 多任务版本

### 主要性能指标

**作者归属任务（Authorship Attribution）**：

| 方法 | F1-Score | Precision | Recall |
|------|----------|-----------|--------|
| DetectGPT | 62.3% | 58.7% | 66.4% |
| GPTZero | 65.8% | 63.2% | 68.7% |
| BiLSTM-CRF | 71.4% | 69.8% | 73.1% |
| BERT-CRF | 76.2% | 74.5% | 78.0% |
| GPT-4 (zero-shot) | 68.9% | 72.1% | 65.9% |
| HACo-Det (单任务) | 81.3% | 79.6% | 83.1% |
| **HACo-Det (多任务)** | **84.7%** | **83.2%** | **86.3%** |

**边界检测任务（Boundary Detection）**：

| 方法 | F1-Score | Precision | Recall |
|------|----------|-----------|--------|
| BiLSTM-CRF | 58.4% | 55.2% | 62.1% |
| BERT-CRF | 64.7% | 62.3% | 67.3% |
| HACo-Det (单任务) | 72.1% | 69.8% | 74.6% |
| **HACo-Det (多任务)** | **76.3%** | **74.1%** | **78.7%** |

**修改检测任务（Modification Detection）**：

| 方法 | F1-Score | Accuracy |
|------|----------|----------|
| BERT-CRF | 61.2% | 68.4% |
| HACo-Det (单任务) | 68.9% | 74.2% |
| **HACo-Det (多任务)** | **73.5%** | **79.8%** |

### 关键发现

1. **多任务学习的优势**：多任务版本相比单任务版本在所有子任务上均有显著提升（+3-5%），说明三个子任务之间存在互补信息

2. **传统检测器的局限性**：传统MGT检测器（DetectGPT, GPTZero）在细粒度检测任务上表现不佳，F1-Score仅为62-66%

3. **大模型的零样本能力**：GPT-4通过prompt进行零样本检测的效果（68.9%）优于传统检测器，但仍显著低于专门训练的模型

4. **修改检测的挑战性**：修改检测任务最具挑战性，最佳F1-Score仅为73.5%，说明区分原始AI生成内容和人类修改后的内容非常困难

### 消融实验分析

论文进行了详细的消融实验，验证了各个组件的贡献：

1. **编码器选择**：DeBERTa-v3 > RoBERTa-large > BERT-base
2. **上下文窗口**：使用前后3个句子的上下文信息可提升2.3% F1-Score
3. **任务权重**：最优配置为 λ₁=0.3, λ₂=0.5, λ₃=0.2
4. **数据增强**：使用不同LLM生成的混合数据可提升泛化能力（+4.1%）

## 优势与创新点

### 优势

1. **任务定义的前瞻性**：首次系统性地定义了人机协作写作场景下的细粒度检测任务，填补了该领域的空白

2. **数据集的高质量**：HACo-Bench是首个细粒度标注的人机协作数据集，覆盖多个领域和多种LLM，具有较高的标注一致性（Kappa=0.82）

3. **框架的统一性**：HACo-Det将三个子任务整合到统一框架中，通过多任务学习实现性能提升

4. **实验的全面性**：对比了多种基线方法，进行了详细的消融实验和错误分析

### 局限性

1. **句子级粒度的限制**：当前方法仅支持句子级检测，无法处理句子内部的混合编辑（如人类修改句子中的部分词语）

2. **修改检测性能有限**：修改检测任务的F1-Score仅为73.5%，在实际应用中可能不够可靠

3. **数据集规模**：9,792个样本相对较小，可能限制模型的泛化能力

4. **计算成本**：需要使用大型预训练模型（DeBERTa-v3），推理成本较高

5. **跨域泛化**：论文未充分探讨模型在未见过的领域或LLM上的泛化能力

## 与当前研究的关联

### 与模型家族溯源研究的关系

本文与[[模型家族溯源]]研究存在以下关联：

1. **共同挑战**：两者都面临**私有微调模型**的检测难题
   - HACo-Det关注人类修改后的AI文本检测
   - 模型家族溯源关注私有微调模型的归属识别

2. **互补性**：
   - HACo-Det提供**细粒度检测能力**（句子级归属）
   - 模型家族溯源提供**模型级归属能力**（识别生成模型的家族）
   - 两者结合可实现"哪个模型生成了哪些句子"的完整溯源

3. **方法论启发**：
   - HACo-Det的多任务学习框架可借鉴到模型家族溯源中
   - 可考虑增加"修改检测"作为模型家族溯源的辅助任务

### 与其他相关研究的联系

1. **与VERITAS的对比**：
   - VERITAS关注风格不变性特征，HACo-Det关注序列结构特征
   - 两者可结合：VERITAS提供鲁棒的文本级检测，HACo-Det提供细粒度定位

2. **与MoSEs的关联**：
   - MoSEs使用风格专家混合模型，HACo-Det使用多任务学习
   - 都强调了多维度特征的重要性

3. **对学术诚信检测的启示**：
   - 可应用于检测学术论文中的AI辅助写作部分
   - 有助于制定更精细的AI使用规范

## 未来研究方向

基于本文的研究成果和局限性，可以探索以下方向：

1. **更细粒度的检测**：
   - 从句子级扩展到词级或短语级检测
   - 识别句子内部的混合编辑模式

2. **跨模型泛化**：
   - 研究对未见过的LLM（如新发布的模型）的泛化能力
   - 探索零样本或少样本迁移学习方法

3. **与模型家族溯源结合**：
   - 联合训练细粒度检测和模型归属任务
   - 构建"句子级模型归属"数据集

4. **提升修改检测性能**：
   - 引入对比学习，学习原始文本和修改文本的差异表示
   - 利用编辑距离等显式特征增强模型

5. **实际应用场景验证**：
   - 在真实的学术写作、新闻编辑等场景中测试模型
   - 研究用户接受度和实用性

## 总结

HACo-Det是首个系统性研究人机协作写作场景下细粒度MGT检测的工作，具有重要的理论和实践价值：

**理论贡献**：
- 定义了新的研究任务（边界检测、作者归属、修改检测）
- 提出了统一的多任务学习框架
- 构建了高质量的细粒度标注数据集

**实践意义**：
- 为学术诚信监管提供技术支持
- 有助于制定更合理的AI辅助写作规范
- 可应用于内容审核、版权保护等场景

**核心启示**：
- 随着AI辅助写作的普及，传统的二分类检测已不足以应对实际需求
- 细粒度检测是未来MGT检测研究的重要方向
- 多任务学习可以有效利用任务间的互补信息

## 关键要点

1. **新任务范式**：从"是否AI生成"转向"哪些部分由AI生成"
2. **数据集价值**：HACo-Bench填补了细粒度标注数据的空白
3. **多任务优势**：联合训练三个子任务可提升3-5%性能
4. **挑战性任务**：修改检测是最困难的子任务（F1=73.5%）
5. **应用前景**：可与模型家族溯源结合，实现完整的文本溯源链

## 相关论文

### 直接相关

- [[模型家族溯源]]：私有微调模型的家族归属识别
- **DetectGPT**：基于扰动的零样本检测方法
- **GPTZero**：商业化的AI文本检测工具

### 方法论相关

- **VERITAS**：风格不变性的MGT检测
- **MoSEs**：风格专家混合模型
- **GLTR**：基于统计特征的检测方法

### 应用场景相关

- 学术诚信检测相关研究
- 内容审核与版权保护
- 多智能体协作系统（[[多智能体协作综述]]）

---

**阅读日期**：2026-01-17
**标签**：#AI生成文本检测 #人机协作 #细粒度检测 #多任务学习 #序列标注