
现在我希望做一个研究，关于大模型生成文本检测与溯源，主要解决一个问题：由于现在开源小模型越来越多，sft, full-fintune，lora，等技术越来越普及，私有微调小模型变得越来越简单。现在市面上的检测器，大都是根据主流通用模型像gpt，claude，gemini，grok，deepseek收集的数据，来训练检测器，但是对于私有微调过的模型生成文本，检测性能可能会下降。同时私有微调模型可能也不好管理，大公司的通用大模型可以通过添加水印等方来监管，但是私有微调模型就没办法强制水印监管，难以溯源。第二个是，对于私有微调过的文本，我们希望建立溯源机制。溯源模型家族，模型家族的定义：基于同一个基座训练出来的模型称为一个家族模型（举一个例子：Qwen家族，Qwen2.5-7B-base，Qwen2.5-7B-finance，Qwen2.5-7B-medical，Qwen2.5-7B-math。这是一个分支，Qwen2.5-32B-base，Qwen2.5-32B-finance，Qwen2.5-32B-math。这是一个分支，但他们都属于同一个模型家族。只是同一个分支血缘关系更近，不同分支的血缘关系更远。）现在我搜集了7个基本家族基座模型（Qwen，gemini，deepseek，phi，mistral，GLM，llama）包含不同尺寸（3B，7B，14B），然后在fiancé，medical，reddit这三个数据集上进行lora微调训练和full- fine-tuning训练，用于模拟现实场景中的私有微调模型。现在我希望你帮我调研一下，我希望构建这个家族模型的度量关系。有什么样的方案，或者你有什么建议我们讨论一下。



## 背景：

随着开源大语言模型（LLM）的普及和微调技术的成熟（如lora、full-fintunig ），私有微调小模型变得越来越简单。这一趋势对现有的AI生成内容（AIGC）检测生态系统构成了严峻挑战。目前主流的检测器大多基于对通用模型（gpt，claude，gemini，grok，deepseek）生成文本的训练，当面对海量的、未知的、经过特定领域数据（如金融、医疗）微调的私有模型时，其检测性能显著下降，甚至失效。更严峻的是，这些私有模型游离于大型科技公司的监管体系之外，无法强制实施水印等溯源技术，形成了潜在的监管盲区，为虚假信息、学术不端和恶意滥用等行为提供了温床。


## 目标：

在这个背景下我们希望建立溯源机制，为了：
一、针对私有微调模型生成文本，提升检测器性能
二、建立家族模型溯源机制，为后续监管制度的建立基础

> 我们定义家族模型为所有基于同一尺寸的基座模型（Base Model）通过不同方式（如不同微调数据、不同微调方法）衍生出的模型集合。

为了实现这一目标，本研究提出一个核心概念：模型家族度量关系（Model Family Relationship Metric）‍。这是一种量化模型之间“血缘”远近的度量体系。通过构建这一度量，我们不仅可以判断一个未知文本是否由机器生成，更有潜力指认其最有可能的“祖先”——即它属于哪个模型家族。

## 核心方法：

- 组织 7 × 3 × 3 × 2= 126 种私有微调模型的生成文本
- 将文本映射到能够反映模型“血缘”的向量空间
- 采用度量学习
	- Siamese/Triplet 网络
	- **层次化 Triplet Loss**：  
		- 正例 = 同分支（如 Qwen‑7B‑finance 与 Qwen‑7B‑medical） 
		- 负例 = 跨家族（如 Qwen‑7B‑finance 与 Llama‑7B‑reddit）
	 - **多任务头**：  
		 ① 二分类检测（机器‑人）  
		 ② 家族分类（7 类）  
		 ③ 分支细分（同一基座的不同尺寸）
	- 损失函数：
		-  L_total = λ₁·L_det + λ₂·L_fam + λ₃·L_triplet
- 性能测试：
	- **ROC‑AUC**
	- **Top‑k Accuracy**（家族/分支归属）
## related work


| 序号  | 题目                                                                                                      | 作者（主要）                                                                                                                                   | 出版年份                                                                                                                               | 关键思路/技术路线                                                                               | 备注                                                         |
| --- | ------------------------------------------------------------------------------------------------------- | ---------------------------------------------------------------------------------------------------------------------------------------- | ---------------------------------------------------------------------------------------------------------------------------------- | --------------------------------------------------------------------------------------- | ---------------------------------------------------------- |
| 1   | **LLMmap: Fingerprinting For Large Language Models**                                                    | Dario Pasquini，Evgenios M. Kornaropoulos【RSAC Labs】【George Mason University】                                                             | 2024（arXiv 2407.15847）                                                                                                             | **主动指纹**：向目标应用发送精心设计的查询，依据模型对这些查询的响应特征（如概率分布、生成风格）在黑盒环境下辨别具体 LLM 版本。                    | 采用统计‑学习方法，兼容多模型集成场景。                                       |
| 2   | **ProFLingo: A Fingerprinting‑based Intellectual Property Protection Scheme for Large Language Models** | Heng Jin Chaoyu Zhang 【Virginia Tech, Arlington, VA, USA】                                                                                | 2024（arxiv2405.02466）0                                                                                                             | **黑盒指纹**：无需访问模型内部权重或训练过程，仅通过对外部 API 的一系列输入‑输出对进行特征抽取，构建唯一的“指纹签名”。                       | 首个专门用于 IP 保护的非侵入式指纹方案。                                     |
| 3   | **Copyleaks’ Innovative Text Fingerprinting Research Uncovers Key Insights on AI Model Reliances**      | Shai Nisan、Yehonatan Bitton、Elad Bitton（Copyleaks 团队）                                                                                    | 2025‑03‑03（[技术报告](https://copyleaks.com/wp-content/uploads/2025/03/Detecting_Stylistic_Fingerprints_of_Large_Language_Models.pdf)） | **风格指纹**：基于大规模文本风格特征（n‑gram、句法结构、词向量分布）对生成文本进行相似度比对，揭示不同模型之间的“风格重叠”。                    | 通过对 DeepSeek‑R1 与 OpenAI 文本的 74% 风格相似度进行实证，展示了模型训练数据的潜在共享。 |
| 4   | **UTF: Under‑trained Tokens as Fingerprints – A Novel Approach to LLM Identification**                  | Jiacheng 、CaiYangguang Shao【Shanghai Jiao Tong University，Northwestern University，University of Chinese Academy of Sciences】             | 2024（[ACL](https://aclanthology.org/2025.llmsec-1.1/))                                                                             | 利用 **未充分训练的 token**（低出现频率词）作为隐蔽标记；通过统计这些 token 的出现概率即可在黑盒环境下辨别模型来源。                     | 该方法无需额外查询，仅通过一次生成文本即可完成检测                                  |
| 5   | **REEF: Representation Encoding Fingerprints for Large Language Models**                                | Zhiguang Yang, Hanzhou Wu                                                                                                                | 2025（ICLR）                                                                                                                         | 通过在模型的 **中间表示**（Transformer 隐层）上学习可区分的编码向量，形成 **可验证的指纹**；指纹在微调后仍保持稳定，适用于模型版权追踪。         | ICLR 2025 口头报告，实验覆盖 Llama‑2、Qwen‑2、Claude‑3 等主流模型          |
| 6   | **Hey, That’s My Model! Introducing Chain & Hash, an LLM Fingerprinting Technique**                     | Mark Russinovich, Ahmed Salem【Microsoft Azure】                                                                                           | ICLR 2026 ([arxiv](https://arxiv.org/pdf/2407.10887))                                                                              | 将模型每一层的权重哈希值 **链式连接**，形成全局唯一的 **Hash‑Chain**；在推理阶段通过少量随机查询验证链的完整性，实现 **高效、可扩展的模型身份认证**。 | 适用于大规模模型库的快速比对，已在 GPT‑4、Llama‑3 上验证                        |
| 8   | **Fingerprinting via Prompt‑Response Distributions**                                                    | Yubo Wang, Xueguang Ma, Ge Zhang, Yuansheng Ni, Abhranil Chandra, Shiguang Gu, Weiming Ren, Aaran Arulraj, Xuan He, Ziyang Jiang, et al. | NeurIPS 2024                                                                                                                       | 设计 **统计指纹**：通过一组精心构造的查询，收集模型对每个选项的概率分布，使用 **KL‑散度** 与 **CKA** 进行层级匹配，实现黑盒环境下的模型身份判别。    | 适用于 API 调用监测与版权追踪。                                         |




## 目前溯源类文章分为两类：
- 水印
- 模型指纹
	- 查询式
	- 风格指纹，硬分类
	- 模型权重层
### 模型指纹类：
#### LLMMap：
	- 查询策略：元信息查询（询问模型的训练集大小，最后更新日期），标语查询（直接问你是谁？），对齐异常利用（利用模型在处理有害信息或争议话题时的对齐反应，不同模型的拒绝格式和语气通常是版本特有的）
	- 推理方案：
		-  闭集识别（Closed-Set）：从已知的 42 个模型版本中进行分类，准确率超过 95% 。
		-  开集识别（Open-Set）：通过对比学习（Contrastive Learning）生成向量化“特征签名”，用于探测数据库中尚未包含的新模型 。
			- 【具体操作】当攻击者向目标应用发送一组特定的查询（Trace）后，这些响应会被输入到一个推理模型f中。如果是新模型，将生成的签名与数据库中的已有签名进行Cosine Sim比对



### ProFLingo：【针对IP知识产权保护】
这篇文章解决问题：如何判断一个黑盒 LLM 是否是从某个开源模型 fine-tune 得来的？

1. 原理：通过构造诱导查询，让原模型+微调模型更容易产生某些错误但一致的回答，而其他无关模型不会，如果一个嫌疑模型在这些查询上的目标命中率（TRR）明显偏高，就说明他很可能是从该原模型微调而来。
【fine-tuned模型TRR极高，unrelated模型TRR极低，而且***最低的派生模型 TRR，仍 > 最高的无关模型 TRR × 3***】
【同源模型，使用相同的tokenizer，高度重叠的数据，相似的架构+训练recipe，可能导致在语意义空间决策边界的相似性】
![[截屏2025-12-18 01.10.25.png]]


2. 方案架构：
![[截屏2025-12-18 00.28.20.png]]
> 关键问题，如何生成prefix呢，从而构造prefix+query，使得模型能回答出Target（错误答案）
> 随机从词表里，随机采样subword，构造token_L=k_length(32tokens)。【这里的subword来自于源模型的tokenizer的词表过滤】
>   x_a​=x_r || encode("simply answer:") || encode(q) 
>   [x_r就是随机采样的prefix token,x_a就是构造的prefix]
>   再将x_a嵌入prompt template h：
>   h[x_a] =f""" Human: [prefix + simply answer + question]
>          Assistant："""
>    优化目标：Loss = max​_π(x_t​∣h[x_a​]) （原文这里用的是多模版联合损失，使得模型对模版不敏感）
>    梯度下降来学习，根据梯度方向，来替换token。生成最好的prefix。


这里放一个生成出的真实query：
query：igoavenquestcionesTasksINDusztusrequesttotDEFRe
sponsecolonANelnorteRepTrueWhereDIRtokenTheS
urYouWriteLinealignigeAlSRahrenENDExpressatives
simply answer: Where does the sun rise?

output：north

**因此看完了这里的prefix构造原理，可以总结其实prefix就是对原模型决策边界的一个“脆弱点探针“**


#### 缺点：
- 【本文明显没有区分lora/full-fintuned】
- 【TRR任可能被极重度fine-tuning削弱】微调的程度对抗攻击，原文倒是提到，微调程度继续加深，TRR可能会快速下降，但是微调加深程度后期下降极慢。 同源但非fintuned也会有较高的TRR（在LLaMA-7B上生成的Query，对LLaMA-13B也会有较高的TRR，可能导致误判）
- 【Query生成成本极高（1.5h/query）】



### Copyleaks【Unexpected Family Resemblence】

![[截屏2025-12-18 01.17.37.png]]一些较新的模型，如 phi-4 和 Grok-1，展现出完全独特的写作风格，与任何主流 AI 模型都不匹配。Mixtral 模型在写作风格上与 OpenAI 和 Llama 有一些相似之处，但仍然保持了自己独特的风格。DeepSeek-R1 模型的编写风格与 OpenAI 的模型非常相似。

这里的Copyleaks提到了家族模型但是目前只做了：Claude, Gemini, Llama, and OpenAI.这四个主流的模型。

#### 核心论据：LLM有风格指纹（Stylistic Fingerprints）
这里的风格指纹来源于：
- 训练语料分布
- 模型架构
- 对齐方式（RLHF/RLAIF）
- 解码偏好（句法，连接词，抽象层级）

> 即使你让模型“模仿不同写作风格”，LLM 仍然会泄露其内在的统计与语言风格指纹

#### 核心方法：
使用三个不同的分类器进行投票
不同的架构--->学不同stylistic的视角
技术报告里并未公布和开源具体训练方案

#### 缺点：
- 目前只分类四个LLM家族：claude，Gemini，LLaMA，OpenAI，no-agreement（others）
- 如果是MoE+instruction tuning是否会带来混合风格影响，DeepSeek-R1，74.2%被误判为OpenAI。MIstral 65%no- agreement ，26%OpenAI,8.8%LLaMA
