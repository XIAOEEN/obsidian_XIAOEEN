# AI生成文本检测与模型溯源论文表格（Markdown）

| 标题                                                                                                         | 会议/期刊                                | 发表时间 | 作者/机构                                                                                                                                            | 链接                                                 | 摘要                                                                                                                                                                                                                                                                                                                                                                                                                           | 摘要（中文）                                                                                                                                                         |
| ---------------------------------------------------------------------------------------------------------- | ------------------------------------ | ---- | ------------------------------------------------------------------------------------------------------------------------------------------------ | -------------------------------------------------- | ---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | -------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| Sandcastles in the Storm: Revisiting the (Im)possibility of Strong Watermarking                            | ACL 2025 (Long)                      | 2025 | Fabrice Harel-Canada; Boran Erol; Connor Choi; Jason Liu; Nanyun Peng; Amit Sahai; Gary Jiarui Song; University of California, Los Angeles       | https://aclanthology.org/2025.acl-long.1436.pdf    | Watermarking AI-generated text is critical for combating misuse. Prior random-walk erasure attacks rely on rapid mixing and perfect quality oracles. Large-scale experiments and human assessments show mixing is slow and oracles falter, making automated removal underperform. Findings challenge inevitability of watermark removal and call for stronger methods and realistic attack models.                           | 对AI生成文本进行水印标记对于遏制滥用至关重要。既有的随机游走删水印攻击依赖于“快速混合”和“完美质量判别器”两项假设。大规模实验与人工评估表明，混合过程实际上很慢、质量判别器也常出错，导致自动化去除水印的效果不佳。该研究质疑水印必然可被移除的观点，呼吁更强的水印方法与更贴近现实的攻击模型。             |
| Can LLM Watermarks Robustly Prevent Unauthorized Knowledge Distillation?                                   | ACL 2025 (Long)                      | 2025 | Leyi Pan; Aiwei Liu; Shiyu Huang; Yijian Lu; Xuming Hu; Lijie Wen; Irwin King; Philip S. Yu; Tsinghua University; Zhipu AI; HKUST(GZ); CUHK; UIC | https://aclanthology.org/2025.acl-long.648.pdf     | Investigates whether student models can acquire teacher capabilities via distillation while avoiding watermark inheritance (radioactivity). Proposes pre-distillation removal (untargeted/targeted paraphrasing) and post-distillation strategies; evaluates robustness of watermark radioactivity under adversarial actors.                                                                                                 | 研究学生模型是否能通过知识蒸馏获得教师模型能力，同时避免继承文本水印（“放射性”）。提出蒸馏前移除（非定向/定向释义）与蒸馏后处理两类策略，并评估水印放射性在对抗行为下的稳健性。                                                                      |
| Improved Unbiased Watermark for Large Language Models (MC MARK)                                            | ACL 2025 (Long)                      | 2025 | Ruibo Chen; Yihan Wu; Junfeng Guo; Heng Huang; University of Maryland, College Park                                                              | https://aclanthology.org/2025.acl-long.1005.pdf    | Introduces MC MARK, an unbiased multi-channel watermark partitioning vocabulary segments to promote token probabilities via keys. Preserves original distribution, improves detectability and robustness over prior unbiased watermarks, with >10% detectability gains in experiments.                                                                                                                                       | 提出MC MARK：一种无偏的多通道文本水印方案，通过将词汇划分为若干片段，并基于水印密钥提升片段内token概率。该方法在不改变模型原始分布的前提下，较既有无偏水印显著提升可检出性与鲁棒性（实验证明检测率提升超过10%）。                                               |
| I Know You Did Not Write That! A Sampling Based Watermarking Method for Identifying Machine Generated Text | GenAIDetect-1 Workshop 2025          | 2025 | Kaan Efe Keleş; Ömer Kaan Gürbüz; Mucahid Kutlu; TOBB ETU; Bilkent University; Qatar University                                                  | https://aclanthology.org/2025.genaidetect-1.11.pdf | Proposes sampling-based watermarking that embeds distinct patterns via token sampling intervention, enabling algorithmic identification while keeping text coherent and natural; aims to mitigate harms like misinformation and plagiarism by reliable detection of LLM-generated text.                                                                                                                                      | 提出基于采样的文本水印方法：在生成阶段干预token采样以嵌入可识别的特征模式，使算法能够检测，同时保持文本连贯与自然。目标是在可靠识别LLM生成文本的前提下，缓解诸如虚假信息与抄袭等风险。                                                                |
| EverTracer: Hunting Stolen Large Language Models via Stealthy and Robust Probabilistic Fingerprint         | EMNLP 2025 (Main)                    | 2025 | Zhenhua Xu; Meng Han; Wenpeng Xing; Zhejiang University; Binjiang Institute of ZJU; GenTel.io                                                    | https://aclanthology.org/2025.emnlp-main.358.pdf   | Gray-box fingerprinting that repurposes membership inference attacks defensively, injecting ownership signals via memorization and verifying through calibrated probability variation. Robust against input and model-level modifications; practical for securing LLM IP with stealthiness and resilience.                                                                                                                   | 提出灰盒指纹方法，将成员推断攻击用于防御场景：通过“记忆注入”嵌入所有权信号，并以校准后的概率变化进行验证。该方法在输入与模型层面的多种修改下均表现稳健，具有隐蔽性与韧性，可用于LLM知识产权溯源。                                                            |
| De-mark: Watermark Removal in Large Language Models                                                        | ICML 2025 (Poster)                   | 2025 | Ruibo Chen; Yihan Wu; Junfeng Guo; Heng Huang                                                                                                    | https://openreview.net/forum?id=5dF4mqVVqK         | Presents De-mark, a framework to remove n-gram-based watermarks via random selection probing to assess watermark strength and identify red-green lists. Demonstrates efficiency in removal and exploitation, raising questions on robustness of watermarking schemes.                                                                                                                                                        | 提出De-mark框架：通过随机选择探测评估n-gram水印强度并识别红/绿token列表，从而实现水印移除与利用。实验显示其高效性，同时也引发关于现有水印方案鲁棒性的进一步讨论。                                                                     |
| PRO: Enabling Precise and Robust Text Watermark for Open-Source LLMs                                       | ICLR 2026 (Conference submission)    | 2025 | (OpenReview submission)                                                                                                                          | https://openreview.net/forum?id=neE8pqIqyR         | Embeds watermarks directly into open-weight LLMs by co-optimizing a watermark policy with the LLM and adding perturbation-aware regularization to resist fine-tuning/merging. Reports improved detectability and robustness versus prior methods with minimal quality impact.                                                                                                                                                | 针对开源权重LLM，直接在模型权重中嵌入水印：通过与LLM联合优化的水印策略以及考虑扰动的正则化来抵抗微调/模型合并。实证表明在保持生成质量的同时显著提升水印可检出性与稳健性。                                                                       |
| ReasonMark: Distilling the Thought, Watermarking the Answer                                                | ICLR 2026 (Conference submission)    | 2025 | (OpenReview submission)                                                                                                                          | https://openreview.net/forum?id=T6NVogsXCZ         | Semantic-guided watermarking for reasoning LLMs: separates thinking and answering phases; distills critical reasoning tokens into a principal semantic vector to adapt watermark strength, preserving logical coherence and improving detection robustness with negligible latency.                                                                                                                                          | 面向推理型LLM的语义引导水印：将生成过程划分为“思考阶段”与“答复阶段”，从关键推理token中提炼主语义向量，基于语义相似度自适应调整水印强度，从而在几乎不增加延迟的情况下兼顾逻辑一致性与检测鲁棒性。                                                         |
| Watermarking Autoregressive Image Generation                                                               | NeurIPS 2025 (Poster)                | 2025 | Nikola Jovanović; Ismail Labiad; Tomas Soucek; Martin Vechev; Pierre Fernandez                                                                   | https://openreview.net/forum?id=hVdD72iom4         | Adapts token-level watermarking to autoregressive image models. Identifies reverse cycle-consistency (RCC) as a barrier; proposes tokenizer-detokenizer finetuning to improve RCC and a synchronization layer for robustness to transformations, enabling reliable detection with grounded p-values.                                                                                                                         | 将token级水印技术扩展到自回归图像生成。识别出“逆向循环一致性（RCC）”是关键障碍；通过微调分词器/解码器以提升RCC，并引入同步层以增强对几何/数值变换的鲁棒性，实现基于统计显著性的可靠检测。                                                           |
| Paraphrasing evades detectors of AI-generated text, but retrieval is an effective defense                  | NeurIPS 2023 (Camera-ready on arXiv) | 2023 | Kalpesh Krishna; Yixiao Song; Marzena Karpinska; John Wieting; Mohit Iyyer                                                                       | https://arxiv.org/abs/2303.13408                   | The paper shows that paraphrasing can evade multiple AI-generated text detectors (including watermarking, GPTZero, DetectGPT, OpenAI classifier). Using an 11B paraphrase model (DIPPER), detection accuracy drops dramatically without changing semantics. As defense, they propose retrieval against a provider-maintained database of prior generations, detecting 80–97% paraphrased generations at ~1% false positives. | 论文表明：通过释义可规避多种AI生成文本检测器（包括水印、GPTZero、DetectGPT、OpenAI分类器）。使用11B参数的释义模型DIPPER，在不改变语义的情况下显著降低检测准确率。作为防御，作者提出基于检索的方案：在服务方维护的历史生成库中检索相似序列，可在约1%误报水平下检测80–97%的释义生成。 |
