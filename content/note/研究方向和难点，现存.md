
- 短的序列token如何监测

model-wise methods DetectGPT，sniffer may require longer document as input text（over 100 token）

- supervised methods prone to overfitting to the training data
like RoBERTa fine-tuning,height by [1]
[1 ]Di Jin, Zhijing Jin, Joey Tianyi Zhou, and Peter Szolovits. 2019. Is BERT really robust? natural language attack on text classification and entailment. CoRR, abs/1907.11932.




### 一般论文的写作思路：

Introduction：
1. 介绍AI泛滥导致的危害（错误信息滥用，大语言模型越来越趋同人类的语言风格，模糊区分边界）
2. 介绍现阶段的方法
	- trainging-free：【***leverage intrinsic statistical*** diffenrence to distinguish, trouble with short text】
		- log(P)[model-wise] :DetectGPT, Sniffer, DNA-DetectLLM, DALD, DNA-GPT， Binolucars
		- style-statistical: GPTzero
	- training-based: 【required annoted training data, ***prone to overfitting training data*** 】
		- supervised fine-tuning:RoBERTa-based,
		- matric: Detective 

3. 现存问题的挑战，从traing-free和training-based两个方面来讲
4. 提出方法来解决问题