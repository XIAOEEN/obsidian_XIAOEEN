
### SVM V.S. SVR

|特性|支持向量分类 (SVM/SVC)|支持向量回归 (SVR)|
|---|---|---|
|**主要任务**|分类（离散输出）|回归（连续输出）|
|**核心准则**|最大化两类间的间隔|构建 ϵ-不敏感误差隔离带|
|**典型损失**|Hinge 损失|ϵ-不敏感损失|
|**支持向量**|边界上或内部的样本|隔离带边界上或之外的样本|
|**对噪声态度**|通过软间隔和 C 参数调节|通过线性惩罚提高鲁棒性|

- soft margin:
	- 相较于硬间隔而言，soft margin引入了一个C惩罚项，合页损失（平衡经验风险和结构风险的复合函数)，允许一部分样本违反约束。C越大对错分惩罚越大，间隔越小。C越小，错分惩罚越小，间隔越大。泛化能力更强。
- 关于SVM的对偶求解
$$max\sum a_i - \frac{1}{2}\sum_{i=1}^{N}\sum_{j=1}^{N}a_i a_j y_i y_j <x_i,x_j>$$
KTT约束条件： $\sum_{i=1}^{N}y_i a_i =0, a_i >= 0,i=1...N$

其中$W^* = \sum a_i y_i x_i$
关于b满足：$y_i = w^T * x_i +b$
判别函数： $f(x) = sign(w^T*x +b)$


- 核SVM：因为SVM只能处理线形可分的数据，对于非线形可分的数据，采用核化将高维数据转化为低维数据。因此就可以使用线形SVM处理非线形问题。好处就是不需要增加计算量，只需要将对偶问题中的点转化为核值即可。
	- 常用的核函数：
		- RBF kernel: $exp\{-\frac{|x-z|^2}{\sigma}^2\}$
		- 拉普拉斯核: $exp\{- \frac{|x-z|}{\sigma}\}$
		- sigmod 核:  $tanh(B^T*X+\theta)$


- SMO：序列最小优化，是为了高效解对偶问题优化，因为SVM的对偶转化为QP（凸二次规划）太慢了。尤其对于大规模数据。采用成对求参数的方法，进行优化。
- SVR: SVM在回归任务中的应用，引入一个$\xi$,来找一个范围，只要预测值和真实值的误差在这个范围内$\xi$, 就可以。
	- SVR的支持向量是在这个隔离范围上和外的点。而SVM的支持向量是在分离边界上和内的点，距离分离平面距离最小的点为支持向量，他是为了最大化间隔