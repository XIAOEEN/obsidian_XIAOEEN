
### 统计语言模型
n-gram
最大似然法估计概率

### 数据平滑的方法：
- Additive smoothing （加一法），每一种出现的情况加一（分母）
- 减值法/折扣法（Discounting）修改训练样本中事件的概率和小于1，使样本中（实际出现的）不同事件的概率和小于1，剩余的概率量分配给未见概率。
- 删除插值法：当N-gram无法估计时，使用n-1 -gram来代替，如果还是不行继续减一，基本思想：使用低阶语法估计高阶语法
-

### 模型性能估计

困惑度PPL（perplexity）
对n-gram来说：
$P(t_i) = \prod_{i=1}^{m+1}P(wi | w_{i-n+1}^{i-1})$
其中$P(T) = \prod_{i=1}^{l_T} P(t_i)$

模型p的困惑度可以表示为：

交叉熵： $H_p(T) = - \frac{1}{w_T} log_2{P(T)}$


$PP_pT = 2^{H_p(T)}$
交叉熵取指数----> ppl

### 词向量：
最早是在NNLM阶段提出的，词嵌入：低维稠密向量空间表示。

- NNLM，RNNLM都是训练好一个模型后自然就训练好了初始的词向量。
  对一个词的one-hot编码，然后进去look-up table查表，获取初始的词向量。然后放进去学习，学习目标是n-gram最大似然。
- C&W模型，则是修改了学习任务，从预测n-gram改为输出目标词的分数，根据上下文词来预测中间词的分数。通过构造正负样例<w0,w1,w2,w3,w4> 正；<w0,w1,替换,w3,w4>负例。【C&W 模型的目标函数是求目标词w 与其上下文c 的联合打分，而其他模型均为根据上下文c ，预测目标词w 。】
- C&W 模型在运算速度上优于NNLM模型，但在许多语言学任务上，效果不如其它模型
- CBOW（continue bags of words），Skip-gram；解决NNLM计算复杂度过高的问题，去除hidden layer，去除词序，只保留核心部分（输入层embedding部分权重向量，input layer，output layer），CBOW：通过输入c（上下文词）预测w（中间词）；Skip-gram：通过输入w（中间词），预测上下文词（c）
- google开源了词向量训练工具word2vec，有两种可选择model（CBOW%skip-gram），有优化学习策略（层级softmax，负采样）![[截屏2026-01-26 16.13.22.png]]



### 统计语言模型 V.S. 神经语言模型

- 优缺点：
	- 统计语言模型：解释性强
	- NNLM： 不需要简化n-gram，不需要数据平滑，泛化能力强
- 缺点：
	- 统计语言模型：需要数据平滑，历史简化为n-gram
	- NNLM： 解释性弱

这里RNNLM 对 DNN（DNNLM）的改进就是 不需要再做对历史做n-gram简化了，或者能学习到更长的问题，长问题效果好。