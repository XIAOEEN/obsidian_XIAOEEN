###  考点：

![[22d9512b79a1aa48b624f23c79cf560f.jpg]]
- [x] NLP的各类范式及区别
- [x] 生成式模式的编解码原理、评价指标(PPL,BLUE,ROUGE)
- [x] 循环神经网络语言模型、注意力机制、Transformer的原理和训练机制、BERT
- [x] 模型优化方式(BP, )、梯度消失、梯度爆炸
- [x] 大模型指令学习、上下文学习、思维链等概念，各类训练方式
	- [ ] 提示学习：
- [x] 事件抽取概念和子任务
	- [x] 实体识别与抽取
	- [x] 实体关系抽取
	- [x] 事件抽取
- [ ] 复杂命名实体识别任务典型模型
	- [ ] BIE，BiLSTM+Attention（RNN+Attention），Bert
- [ ] 属性级情感分析概念及建模方法
- [ ] 任务型对话方法设计思想及典型模型






序列生成的分布
关于生成模型的编解码器 主要是解码
建议复习 里面有一些 
**会考bpe**
就考rnn  cnn dnn不考
不考词向量
马尔可夫不考
模型后期怎么优化，梯度下降 优化方法有什么（会考细节 bert）
不考bart
知识蒸馏  偏好对齐会考，后面的任务可能会用到
给场景，问用什么样的模型
任务型对话四个模块
训练策略和优化方法不考
**序列标注**
各类训练方式中考了一种训练方式
训练微调（考三体？？并行）
Transformer的并行方式 并行训练机制
第六章第三节
强化学习不考
各个范式的区别
掩码机制不考
解码的什么算法不考
kqv计算不考
不考模型的区别
归一化和残差连接不考
属性情感分析 联合分析考
transformer解码的搜索算法不考
换激活函数不算
考指针
提示学习不考
大模型的训练方式看一看
考的面向某个任务，
考的具体第几类范式
机器学习的那些
训练策略考
扩散模型不考
rnn attention transformer比较重要

**一、题型**

1. 填空题20道，每道1分；
    
2. 简答题6道，每道7分；
    
3. 综合题38分（计算、设计等）
    

**二、考试内容**

1. 填空题

（1）卷积层作用是什么

        通过卷积操作减少参数

（2）池化层作用是什么？

    通过采样减少网络规模

（3）Prompt核心思想是什么？

    更好更充分的使用预训练模型，重构下游任务以迁就预训练模型。

（4）什么是软注意力？
-  **软注意力**的本质是一个**连续的概率加权机制**。它通过 Softmax 函数将注意力焦点“平滑”地分布在整个输入序列上，使得模型能够以可微的方式学习到输入与输出之间的细粒度关联。
- 软注意力为所有输入分配 0 到 1 之间的连续权重。相比之下，硬注意力则是直接从输入中**找到某个特定单词**进行对齐，而将其余所有单词的概率硬性设为 0
	

（5）RNN训练方法是什么？
	  BPTT，随时间反向传播算法
	  [在使用 BPTT 进行训练时，由于激活函数导数的连乘效应，RNN 经常会遇到 **梯度消失 (Gradient Vanishing)** 或 **梯度爆炸 (Gradient Exploding)** 的问题，导致模型难以建模长距离的依赖关系]

（6）CBOW是训什么？
	 词向量，根据上下文词来训练预测当前中间的Target word
	 【**输入层是目标词上下文各词词向量的平均值**，输出层则是该目标词的概率分布】

（7）SkipGram是训什么？
		根据当前词向量（target word）来预测上下文词（context）
	【它的**输入层是目标词本身的词向量**。在训练过程中，模型每次从目标词的上下文范围内选取一个词作为预测对象，输出层计算该上下文词】
（8）激活函数有什么作用？（softmax，sigmod，输出大于0和ReLU，大于0时恒为1，tanh ，输出-1到1之间）
	增强网络表达能力：对神经网络引入非线性因素对变量进行非线性变换
	支持网络参数可学习：激活函数往往连续可导，支持反向传播算法求梯度

（9）随机梯度下降与梯度下降的异同？
      随机梯度下降是对每一条sample都进行梯度更新，梯度下降是输入整个batch数据，然后进行梯度更新

（10）Attention中普通模式与键值对模式的异同？
- **相同点：**
	input都是Query向量和参考集空间（key，K）的加权求和机制
	1. 打分阶段：$s_i = f(Q_i,K_i)$
	2. 归一化：$a_i = sofmax(s_i)$ 获取权重分数
	3. 加权求和：根据权重对目标向量进行累加，得到最终的Attention value
- **不同点：**
	普通模式：参考集合仅有一组向量构成$\{K_1, K_2, K_3....\}$
	$Att-V = \sum{a_iK_i}$
	匹配特征（K）和提取的语义特征时一致的
	key-value模式：参考集合由(key,K)和（value,V）两个部分组成
	$Att-V = \sum{a_iV_i}$
	实现了匹配特征（K）与内容特征（V）的分离，使得模型能更灵活的学"去看哪里"和“提取么”

（11）什么是BERT？
**Bidirectional Encoder Representations from Transformers**
基于 Transformer 的双向编码器表示

（12）什么是BART？
Bidirectional and Auto-Regressive Transformers
Seq2Seq的框架
一种基于 **Transformer 的编解码（Encoder-Decoder）** 结构的预训练模
通过将损坏（含有噪声）的文本作为输入，并训练模型去还原原始文本，从而学习语言规律
（13）GPT使用的是Transformer的什么？
Decoder

（14）CRF的作用是什么？
      综合利用上下文信息，并建立输出之间的关系。

（15）BERT中为什么要使用位置嵌入？
【BERT 中使用位置嵌入是为了**弥补自注意力机制（Self-Attention）对位置感知的缺失**】
- self-Attention本质上是一种词袋模型（bags of words）， 这意味着，如果不引入，位置信息，模型将输入序列是为一组无序的词集合，无法区分不同序列句子的语义。
- 让模型可以学习到词语之间的距离关系和距离依赖。
- 构建完整的输入表示，BERT的输入是Token embdding+ Segement embedding+ Position Embedding。
- 在并行计算与序列顺序之间取的平衡。模型可以并行计算每一个位置token的注意力，同时保持序列顺序信息。
    
    

（16）什么是曝光偏差？
模型在训练阶段只“见过”完全正确的真实数据，这就导致**模型生成的分布与真实的数据分布并不严格一致**

因为：训练阶段，未来加速训练并保证稳定，模型在第t步的输入是来自训练集中的真实前缀序列，而不是模型自己生成的词，这种方式叫做特teacher forcing.
但是在推理阶段，模型没有实际参考词汇，在第t步输入必须是模型在t-1步生成的词
曝光偏差本质上是**训练和预测之间的断层**。训练时模型被“老师”手把手教（Teacher Forcing），但是推理阶段完全依赖自己。
解决方法：scheduled sampling（计划采样）
- 混合训练：在训练阶段，不完全依赖真实数据，而是混合使用真实数据和模型生成数据。
- 动态调整，在训练的后期，逐步增大模型接触自己生成数据

（17）生成任务中常用的评价指标是什么？
BLEU，ROUGE，
Perplexity， ROUGE-L，NIST

（18）二范式的特点是什么？
人工标注，神经网络自动提取特征

（19）梯度消失的原因是什么？
激活函数的导数<0, 在反向传播的时候，连续的导数小于0的值相乘导致梯度消失

（20）prompt适合什么语言模型？
大型预训练语言模型

2. 简答题

（1）Attention是什么？其在NLP领域有什么应用？优势是什么？
- 注意力机制本质是一个加权求和的模块，用来在节点之间建立关联关系。
	- 分三个阶段：
		- 打分阶段 计算查询向量Q与 参考集合（Key，K）之间的相关性分数
		- 归一化阶段：通过softmax 函数将分数转换为权重概率分布
		- 加权求和：$Att-V = \sum{a_i*V_i}, a_i = softmax(Q_i*K_i)$
- 其中普通模式（k=v）, 键值对模式（Q，K，V分离，参考集合由（k, V）组成）
	 其中软注意力（soft attention，连续的概率分布）,硬注意力（hard Attention，硬性选择某一个词）之分.Attention是transformer的重要组成部分，其中self-attention机制能学习序列内的词与词之间的复杂关系。	     
***优势：***
- 解决长距离依赖问题，相比RNN，注意力机制更有效的捕获序列中距离较远的元素之间的关系，提升任务性能。
- 支持并行计算，客服RNN无法并行计算的问题
- 无监督信号


（2）指针网络与指针生成网络的异同

（3）第三范式与第四范式的异同
- 相同点：
	- 都有预训练模型
- 不同点：
	- 第三范式主要是预训练+下游任务微调。通过少量的标注数据，对预训练模型来做微调，以适配下游任务。需要改变模型权重。
	- 第四范式由于scalling low 大量预训练，通过prompt engineering 不改变模型权重，通过定义下游任务来适配模型

（4）RNN语言模型与DNN语言模型相比有什么优势
【DNN只能以固定长度的n-gram输入，RNN能学习当前词所有的历史信息。】
- DNN实际是n-gram模型的近视，只能采用上下文固定长度的n元短语作为输入，距离较远的上下文词会被忽略。而RNN可以保留每一个词的全部历史信息。
- 能利用上一个输入的隐藏状态信息，下一个预测与上一个预测的隐藏状态有关。循环利用隐藏状态信息，在输出当前预测的时候同时更新状态信息。加入了时序信息。

（5）GPT与BERT的异同
	GPT使用的是Decoder部分，BERT使用的是Encoder部分
	都使用了Attension机制
	都是预训练语言模型，同属于第三范式：预训练+下游微调
	预训练任务不一样，GPT：NTP+下游任务微调
	BERT：MLM+NSP任务预训练+下游任务文本分类，情感分类

（6）词向量的特点是什么？在NLP领域有什么价值？
	   词向量主要用来表示当前词中蕴含的信息，将词表示到低维稠密空间。在向量空间同义词可以被更好的表示为空间距离，孕含更多的词信息，是后续基于transformer模型的基础。词的相似性可以表示为向量空间的距离

3. 综合题

（1）维特比算法（10分）

123三个状态，ABC三种观测，计算量不大，都是一位小数。每个状态只能转换到自身与后面的状态，一个状态只对应两个观测。比如1状态只能转换到1和2，产生A或B观测。

![](https://i0.hdslb.com/bfs/article/164bc23808629a0491426a3a97fd779b4aea9874.png@1192w.webp)

（2）语言模型

假设输入是 (x1, x2, x3, x4, x5) 的一个序列，写出下列LM的输入与目标输出：

- DNN
    
- RNN
    
- CBOW
    
- SkipGram
    
- GPT
    
- BERT
    

![](https://i0.hdslb.com/bfs/article/aba3b010e2a7c02de3c5889f6c003839cdcd5022.png@1192w.webp)

（3）设计题

设计一个能够处理翻译与摘要任务的model，且含有Attention机制

- 画图，描述输入输出与函数关系；
    

![](https://i0.hdslb.com/bfs/article/6949ddfb16c3d3868c7df29da5f22274a3fae865.png@1192w.webp)

- 训练方法，损失函数是什么损失函数是对数损失。初始化随机权重和偏差，把输入传入网络，正向传播，得到输出值。计算预测值和真实值之间的误差。然后BPTT，对每一个产生误差的神经元，改变相应的（权重）值以减小误差。迭代更新，直到找到最佳权重。
    
- 工作原理是什么将编码-解码+注意力模型和指针网结合，生成**既可产生也可选择的**输出。
    


# **重点知识：** #重点知识 

只想及格的话，背过下面的知识点，会做大题的1，2即可。

常见损失函数：绝对值损失函数，平方损失函数，交叉熵损失函数

CNN的结构：卷积层，池化层，全连接层交叉堆叠

**CNN****结构特性**：局部连接，参数共享，空间或时间次采样

**RNN****有哪些改进和变形**：LSTM，GRU LSTM用来解决RNN的长距离依赖问题 GRU将LSTM的输入门输出门简化为更新门

**激活函数有什么作用？应该具有哪些性质**作用：增强网络表达能力，加入非线性因素性质：连续可导，激活函数和导函数简单，导函数值域范围合理。

**什么是反向传播算法？**将前馈输出的误差以某种形式反传给各层的所有单元，各层按照本层误差修正各单元连接权值。

**什么是梯度下降算法?**误差反向传播时，沿着梯度的负方向调整参数，使得最快达到误差最小的一种方法。什么是梯度消失问题？如何解决？

**梯度消失：**在误差反向传播时，每一层都要乘以激活函数的导数，若 该导数值小于1，将导致误差愈来愈小，甚至消失。（如果导数很大将导致梯度爆炸）解决办法：选择合适的激活函数（Relu），用复杂的门结构代替激活函数，残差结构。

**卷积层：**通过卷积操作减少参数**池化层**：通过采样减少网络规模**全连接层**：将池化层的单元平化

**RNN**  **有什么优点？**DNN,CNN输入输出定长，RNN处理变长问题效率更高 DNN,CNN无法处理时序相关的问题

**相较于BP，BPTT有什么特点？**BPTT损失函数定义为每一个时刻的损失之和，它会在每一个时间步长内叠加所有对应的权重梯度

**概率语言模型的参数学习方法**：最大似然估计常见的离散词表示：one-hot，词袋

**常见神经网络语言模型：**NNLM，RNNLM，C&amp;W，CBOW，Skip-gram

**什么是语言模型？/ 语言模型的思想？**

用数学的方法描述语言规律.用句子S=w1,w2,…,wn 的概率 p(S) 刻画句子的合理性概率

**语言模型存在的问题？**由于参数数量过多问题需要进行词i的历史简化n-gram由于数据匮乏引起0概率问题需要进行数据平滑

**RNN****为什么能解决神经网络语言模型“需历史简化”的问题？**随着模型逐个读入语料中的词，RNN隐藏层实际上包含了此前所有的上文信息，因此不需要简化为n-gram

**什么是词向量？**一些词表示方法（one-hot）导致模型耗空间大，且稀疏，需要构造低维稠密的词向量作为词的分布式表示

**词向量特征**语义相似的词，其词向量在空间距离更相近，相似关系对的词向量之差也相似。

**注意力机制优势？**让任务处理系统找到与当前任务相关显著的输入信息，并按照重要性进行处理。不需要监督信号，可推导多种不同模态数据之间的难以解释、隐蔽性强、复杂映射关系，对于先验知识少的问题极为有效。可以解决长距离依赖问题，提升任务性能。

**隐马尔可夫研究的三大问题**：评估问题，解码问题，学习问题

**隐马尔可夫的五元组：**状态序列，观察序列，状态转移矩阵，观察概率矩阵/发射矩阵，初始状态

**Seq2seq****可以分为：**生成式-序列生成模型，选择式-序列生成模型，选择-生成式-生成模型

**Transformer****在训练过程中采用什么技术实现并行操作**：MASK

**序列生成模型评价指标**：正确率，召回率，BLEU，ROUGE

**什么是马尔科夫模型？**马尔科夫模型是定量描述随机事件变化过程的模型，t时间的状态qt 只与其在时间 t -1的状态相关。

**什么是隐马尔可夫模型？**隐马尔可夫模型是一个双重随机过程，模型的状态转移过程不可观察，可观察事件的随机过程是隐蔽状态转换过程的随机函数、

**Transformer****有哪些特点？**全部采用Attention机制。训练时解码端和编码端都能并行预测时编码端并行，解码端串行具有捕捉长距离依赖的能力。

**Transformer****的结构？**

编码端：6层Attention堆叠，包含2个子层（Multi-head attention 和Feed Forward Network）

解码端：6层Attention堆叠，包含3个子层（Multi-head attention ，cross-attention和 Feed Forward Network）

交叉注意力部分：解码端的每一层与编码端的最后输出层做 cross-attention

**MASK****有什么作用？有几种？**mask通过对某些值进行覆盖，使其不产生效果。

Padding Mask：使不定长序列可以按定长序列统一并行操作。 Sequence Mask：防止标签泄露，只用在decoder中的self-attention。

**序列生成有什么问题？如何解决**

**曝光偏差问题：**模型生成的分布与真实的数据分布并不严格一致，也就是 OOD 问题

Scheduled Sampling，在训练的过程中混合使用真实数据和生成数据训练

与评价目标不一致问题：用强化学习的策略进行模型训练

**第三范式是**：预训练-精调范式，分为Pre-training阶段（大量语料库非监督训练），Fine-tune阶段（针对特定任务的训练集监督学习）是预训练语言模型“迁就“各种下游任务。具体体现就是通过引入各种辅助任务loss，将其添加到预训练模型中，然后继续pre-training，以便让其更加适配下游任务。总之，这个过程中，预训练语言模型做出了更多的牺牲。

**第四范式是：**预训练-提示-预测范式，是各种下游任务“迁就“预训练语言模型。我们需要对不同任务进行重构，使得它达到适配预训练语言模型的效果。总之，这个过程中，是下游任务做出了更多的牺牲。

**指针网络解决了什么问题？**

解决了传统编码-解码架构输出词表大小固定，无法根据输入情况动态变化的问题

**指针生成网络解决了什么问题？**

输出词表与输入词表相同，无法处理输出需要产生输出词表以外词的情况

**拷贝网络解决了什么问题？**遇到OOV（out-of-vocabulary）时，出现表达不准确问题，对输入中的生僻字直接拷贝到输出序列中。

**prompt****的两种形式：**完形填空，前缀提示 。改变任务形式利用预训练模型完成任务（用于小样本学习或半监督学习，某些场景下甚至能做到零样本学习）               

**EMLO****：**学习深层的上下词表示知识，并用此来更好地增强各类NLP任务。

**GPT**：采用transformer的decoder部分   **BERT****：**采用堆叠的双向transformer encoder

**预训练语言模型**是采用迁移学习的方法，通过自监督学习从大规模的数据中获得与具体任务无关的预训练模型，然后用训练好的预训练模型提高下游任务性能的一种数据增强方法。

**预训练语言模型的优势？**

通用知识：利用几乎无限的文本，隐式地学习到通用的语法语义知识

知识迁移：可以将开放域学习到的知识迁移到下游任务，改善低资源任务。

扩展性强：预训练+微调机制有更好的扩展性，支持新任务时，只需要利用该数据的标注任务进行微调即可。

**预训练语言模型分为哪几类？并分别举一个例子。**

自回归：输入上文内容，预测下一个词。例如：GPT。

自编码：根据上下文内容，预测MASK掉的值。例如：BERT。

广义自回归：根据上下文内容，预测下一个可能的单词。例如：XLNet。

**什么是BART？-**

BART是一个用来预训练seq-to-seq模型的去噪自编码器。它通过在输入序列中加入随机的噪声函数，并利用seq2seq结构从损坏的文本中重构原始文本来训练模型。BART运用了基于Transformer的机器翻译结构，可以看作是BERT(encoder结构)和GPT(decoder结构)的结合体。

**GPT** GPT全称为 Generative Pre-Training，即生成式预训练。类似于 ELmo，GPT 是一个两阶段模型，第一阶段 pre-training，第二阶段 fine-tuning。它们之间的区别在于，ELmo 模型的基本框架采用的是双向 LSTM，而 GPT 采用的是 Transformer Decoder 结构。

**BERT****中为什么要使用位置嵌入？**添加该嵌入是为了克服transformer的局限性,与rnn不同,transformer无法捕获“序列”或“顺序”信息