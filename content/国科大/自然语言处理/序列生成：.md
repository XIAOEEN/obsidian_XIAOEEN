
- 自回归序列生成：根据序列历史信息来预测序列的下一个词
- 条件序列生成：根据输入的内容X生成一串特定的序列Y【有监督任务】
- 可控序列生成：根据输入内容X和可控属性生成序列Y

Seq2Seq 序列生成模型分为
- 生成式模型Decoder：根据编码端输入表示和先前时刻的tokens，生成词表的tokens概率分布，并根据该分布输出当前的输出词。）（编码端和解码端的词表可以相同也可以不同，各自有各自的词表，可以处理OOV问题，一般用unk表示）
- 选择式模型Decoder：根据编码端输入表示和先前的tokens，从输入端选择一个tokens作为输出tokens（解码端和编码端的词表相同）
- 生成-选择模型Decoder：前两种方式的结合，输出可以从输入中选择也可以由Decoder端生成。（可以处理oov问题）
- transfomer里使用的mask机制：
	- padding mask：处理非定长问题，使不定长序列可以按定长序列统一操作并行处理。在所有的scaled Dot- product Attention都需要用到
	- Sequence mask：防止标签泄漏，在Decoder里的self-Attention用到。保证解码端使用teacher forcing 并行训练标签不会泄漏。
	- 当batch-size>1时，需要同时使用两个mask机制
	- 推理的时候编码端并行生成输出tensor，解码端串行生成。训练时，编码端和解码端都是并行训练。
- 指针网络pointer network： 
	- 在指针网络里，注意力直接当作输出词概率分布输出，这样就作为原始词的复制。
	- **解决预设词表未登录词（OOV）问题：** 在信息抽取或摘要任务中，经常需要复述原文中出现的生僻人名、地名，指针网络可以直接“复制”这些词，而不受模型预设词表的限制
	- 输出直接从输入中选择，输出词表与输入词表完全相同，无法处理输出词表需要产生输入词表意外的情况。
	- 输出是输入的一个子集，输出的长度动态变化
- 指针生成网络：
	- 基本思想：编码-解码器+attention结合指针网络，输出即可以从输入词表中选择，又可以生成。Yi既可以从X的词表中产生，也可以从输出的Y词表中产生，这样既可以处理高质量摘要，又可以处理OOV问题。
- 自编码：用于无监督学习中的有效编码，通常用于降维。自编码器是一个输入和学习目标项目的网络，有编码器和解码器组成。通常来建立输入空间X和特征空间H的映射F。当学习完毕后，编码器输出的隐藏层特征即为对X的表示。
	- ![[截屏2026-01-27 01.42.38.png]]
		- 普通编码器的问题：无法处理中间区域的生成，普通编码器只用来设计学习精确的重构训练样本，模型无法生成有意义的中间状态。
		- VAE，变分自编码器，通过引入噪音采样机制和正太分布的约束来解决这个问题，使得潜在空间变得连续且平滑。
-
### 序列生成任务的评价指标：

presicion:正确率，预测正确的占测试总数量

recall：召回率，预测正确的数量占全部正确的数量

BLEU（bilingual Evaluation Understudy）生成序列x与参考序列s之间n-gram的重合度，最早用在机器翻译任务。
- 计算方法：在生成序列里每一个n-gram，如果在参考序列里找到了计数1，最后，所有的n-gram统计完毕后，使用计数/生成序列的总n-gram数。
- 会出现一个问题比如生成序列全是同一个词，且在参考文中存在，BLEU就是100%。因此需要修正，修正方法：同一次单词匹配的次数不能超过该单词在**任一单条参考译文中出现的最大次数**（即所谓的截断/Clipped 计数）
- BLEU常常只评估n-gram在参考序列的重合度，因此机器会偏向于生成短文本，文本越短，精度越高（分母越小），因此引入长度惩罚因子。如果候选序列长度短于参考序列就加入惩罚。（$e^(1-ls/lx)$）$l_x$表示候选文本，$l_s$表示参考文本。
- 公式：$$BLUE-U = b(x)\exp(\sum_{i=1}^{N}\frac{1}{N}P_n) $$ 其中b(x)表示惩罚因子，1/N设置为不同的N元组合的权重，一般N取4，$p_n$表示n-gram的概率
- ROUGE：与BLEU不同的是，ROUGE算得是召回率，因此它统计同时出现在生成序列和参考序列中的 n 元词个数，并将其除以**参考序列（Reference）中 n 元词的总数**。同理分子也有截断机制，防止刷分。