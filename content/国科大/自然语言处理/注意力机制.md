
### 注意力机制：
注意力机制本质就是一个加权求和的过程

#### 传统注意力机制：
- Soft-Attention 软注意力机制：求注意力概率分布的时候，对句子中每一个输入都分配一个连续的概率分布（0-1的数值）
- 生成候选集合:
		- 键值对模式(key K，Value，V)
		- 普通模式（Key K）
	- 打分求权重：求K对Q的权，计算Q对于每一个$K_i$的权重 $a_i = f(Q*K_i)$
	- 归一化注意力概率分布：$a_i = softmax(a_i)$
	- 根据权重分配注意力：
		- 键值对模式：$Atten-V= \sum{a_i*V_i}$
		- 普通模式：$Atten-V = \sum{a_i*K_i}$
- hard-Attention 硬注意力机制：求注意力概率分布的时候，直接从句子中挑中某个词，然后把目标词和这个选中的词对齐，其余的词概率对齐为为0。类似于one-hot编码。
- 全局注意力Global-Attention：Decoder端的Attention要考虑Encoder端输入序列的所有词
- 局部注意力Local-Attention：Local Attention本质是soft-attention和hard-attention的一个结合，一般首先预估一个对齐位置pt，在pt位置左右大小为D的窗口范围内取类似于soft-attention的概率分布 。
- 注意力机制的优势：
	1. 让处理系统找到与当前任务相关的显著的输入信息，并按照重要性进行处理，从而提高输出质量。
	2. 不需要监督信号，可推理不同模态数据之间难以解释、隐藏性强、复杂映射关系，对于先验认知极少的问题，极为有效。
	3. 解决长距离依赖问题，提升任务性能。
- 注意力编码：
	- 编码为单一向量：
		- 句向量：句子编码为一个向量
		- 词向量：句子中的每一个词编码为一个向量
	- 编码为一个序列：
		- 不同序列融合为一个序列（将K序列对Q序列编码）
		- 相同序列的自注意力编码（当k=Q的时候）
	- muti-header就是对同一个序列相同的操作做多次，参数不共享，最后concat在一起