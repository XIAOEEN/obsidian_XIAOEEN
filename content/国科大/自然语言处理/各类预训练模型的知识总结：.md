### 常见的预训练模型： #预训练模型
Transformer 2018年出现：
Encoder编码器：token embedding+postion embedding input
- 与$W_q$, $W_k$, $W_v$ 相乘，得到每一个header的Q，K，V
- 每一个header的Q与其他header的K分别相乘，得到的$a_ij = Q_i*K_j$
- $a_ij *V_i$ contact $a_ij*V_j$ 最后送入 Decoder做同样的解码操作
- 只不过Decoder 会接收两个输入，Input token embedding + output embedding form Encoder 【这里的Input embdding 会做Mask Matrix，只允许模型看到前面的token 信息， 防止信息泄漏】
![[截屏2026-01-25 18.01.40.png]]
![[截屏2026-01-25 18.01.58.png]]
![[截屏2026-01-25 18.02.23.png]]

- ***统一序列建模***
- AR model 【无法对上下文信息建模】
- GPT Decoder only
	- 仅采用Mask mutiHeader Attention 结构，考虑上文词预测当前词
	- 预训练阶段使用CLM 语言因果建模任务NTP（next token prediction）
	- 下游任务（classification，Entailment， Similarity， mutiple choice）微调
- AE model 【预训练任务和微调任务（训练和测试阶段的）方式不同，在预训练阶段出现的mask，在下游任务微调的时候不会出现，一般下游任务：情感分类，命名实体识别】
- Bert Encoder only
	- pretrain和Fine-tuning使用相同的模型结构
	- MLM和NSP用于预训练任务
	- MLM 随机mask token，（15%被选中，其中的80%被mask，10%被随机替换为其他的词，10%不动）
	- NSP任务，<A，B> isNext，<A, C> NotNext, 然后建模[CLS]A[SEP]B[SEP], token embedding+position embdding+ segment embedding（标注每一个token属于哪一个句子）input attention
- AR+Bidirectional Information
- XLnet
	- permutation LM 排列语言模型（对输入的序列枚举Ti种可能的排列情况，然后送入attention）使得模型可以对文本双向建模，捕获上下文信息
	- two-stream Attention 双流注意力机制（内容流：包含当前位置的内容信息，查询流：只包含当前位置的位置信息）保证模型可以仅通过当前位置信息预测当前位置内容 
	- 其中使用了两种Attention，内容流使用standard-Attention机制，查询流使用Mask Attention机制，保证当前token看不见后面的information
- T5
	- 统一了自然语言处理的任务都变为text2text任务
	- 使用Encoder+Decoder的机制，Encoder捕获上下文信息，Decoder预测下一个token
	- 其中在预训练阶段修改了MLM机制，采用span corruption（片段破坏） mask，机制，mask连续的一段文本，来学习语言长程依赖和内在联系。
- ***认知建模***
- Transform-XL 引入段级递归机制和相对位置编码等方式解决固定窗口问题。
- 新的预训练任务
- Roberta
	- 在Bert基础上去掉了NSP任务
	- 使用更大SIZE，更大数据集进行预训练
	- 动态修改[MASK]机制：同一条数据可以采用不同的mask方式
- ERNIE1.0
	- 引入两种新的mask策略：短语mask和实体mask
- ERNIE2.0
	- 预训练采用continue Learning（用大量的数据与先验知识构建不同的预训练任务：word-aware，structure-aware， semantic-aware），然后这些预训练任务顺序更新预训练ERNIE模型


### 模型优化: #模型优化
- 系统级优化：
	- **单设备优化**：***混合精度训练***（FP32，使用动态损失缩放操作来摆脱浮点截断），***梯度检查点***（处理冗余激活，只保留存储前向传递后的一部分激活状态来节省内存），***CPU存储参数***（ZeRO-Offload等一些工作来设计安排CPU和GPU内存之间的交换，以便内存交换和设备计算可以尽可能的重叠）
	- **多设备优化**：分布式训练，数据并行和参数并行
- 学习优化算法： 
	- 训练方法：***屏蔽标记***(优化随机mask，在训练过程中根据重要性选择性的屏蔽一些标记学习)， ***预热策略***（训练开始时线形增加学习率，在不同层自适应使用不同的学习率以加快学习速度）， ***共享相似的自注意力模式***（先浅层训练，然后复制构建深层网络，在训练期间删除一些层以降低反向传播和权重更新的复杂度【dropout】）
	- 模型架构： transformer的复杂性（设计低秩内核），结构变体（MoE网络，Router+多个FNN构成动态选择专家模块），注意力（结合全局注意力和局部注意力，长序列被压缩为少量的元素，降低计算复杂度）
- 模型压缩
	- ALBert（瘦身版的Bert）：所有transformer之间参数共享，词嵌入矩阵分解（输入的词嵌入矩阵分解为两个更小的矩阵）， 使用SOP替代NSP

