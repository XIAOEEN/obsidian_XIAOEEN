
在自然语言处理（NLP）领域，范式是指具有特定形式的任务模型框架。根据这些来源，NLP 的发展经历了从基于规则的理性主义到基于数据驱动的经验主义的演变，并最终形成了机器学习背景下的**五大核心范式变迁**。

以下是 NLP 的各类主要范式及其区别的详细分析：

### 一、 NLP 发展的五大机器学习范式

基于机器学习的发展历程，NLP 范式经历了从“特征工程”到“提示工程”的五个阶段：

1. **非神经网络时代的完全监督学习（第一范式，1990~2010）**
    
    - **核心特征：** 依赖**特征工程**（Feature Engineering），需要人工设计和定义大量的特征模板。
    - **主要模型：** 概率模型如朴素贝叶斯（Naive Bayes）、隐马尔可夫模型（HMM）、最大熵模型（MaxEnt）和条件随机场（CRF）。
    - **优缺点：** 可解释性强，但解决问题的方法复杂，劳动强度大且应变能力弱。
2. **基于神经网络的完全监督学习（第二范式，2010~2017）**
    
    - **核心特征：** 转向**结构工程**（Architecture Engineering），利用神经网络自动提取特征，通常采用端到端的解决方案。
    - **主要模型：** 卷积神经网络（CNN）、循环神经网络（RNN，如 LSTM、GRU）、Transformer 和图神经网络（GNN）。
    - **优缺点：** 简化了问题解决过程，效果显著，但需要大量的标注数据（有监督），且可解释性较差。
3. **预训练、精调范式（第三范式，2018~2021）**
    
    - **核心特征：** 重点是**目标工程**（Objective Engineering），引入各种辅助任务损失（loss）使预训练模型（PLM）适配下游任务。
    - **基本思想：** 通过自监督学习从大规模无标注数据中学习通用知识，再利用少量标注数据对特定任务进行微调（Fine-tune）。
    - **代表模型：** BERT、ELMo、GPT-1/2。
4. **预训练、提示、预测范式（第四范式，2021~2022）**
    
    - **核心特征：** 转向**Prompt 挖掘工程**，不改变预训练模型本身，而是通过重新定义下游任务建模方式来适应模型。
    - **基本思想：** 将下游任务转化为“完形填空”等提示（Prompt）形式，直接利用预训练语言模型（LM）的知识完成任务。
    - **优势：** 适用于小样本甚至零样本学习。
    - Prompt Engineering是将原始输入X转化为带有Prompt提示输入的过程
5. **大语言模型范式（第五范式，2023 至今）**
    
    - **核心特征：** 通用预训练大模型（LLM），将所有下游任务**统一为生成式任务**。
    - **技术手段：** 通过提示学习（Prompting）、上下文学习（In-context Learning）、思维链（CoT）和指令微调等方式与人类习惯对齐。
    - **关键规律：** 遵循“缩放定律”（Scaling Law），即系统性能随计算量、模型参数和数据规模的增加呈幂律增长。

---

### 三、 主要区别总结

各类范式的核心差异主要体现在以下三个方面：

|维度|第一/二范式（传统监督学习）|第三范式（精调）|第四/五范式（提示/大模型）|
|:--|:--|:--|:--|
|**工程重心**|特征定义或神经网络结构设计|设计预训练和微调的目标函数|提示语（Prompt）的设计与优化|
|**数据需求**|依赖大量特定任务的标注数据|大量无标注预训练数据 + 少量标注微调数据|极少量任务数据，甚至无需标注（零样本）|
|**任务形态**|每个任务有专门的模型和输出层|共享预训练参数，仅微调任务层|任务被重新定义（如转化为完形填空或生成任务）|
|**解释性**|统计方法解释性强，神经网络解释性差|解释性较差|解释性极差，但在认知推理上有潜力|

总之，NLP 范式的演变是从“让机器适应人工特征”向“让任务形式适应模型已习得的通用知识”转化的过程。